{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Домашнее задание №8</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительный материал для выполнения дз**:\n",
    "- Воронцов К. В. Математические методы обучения по прецедентам. 2012. http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf (разделы 5.2 и 7.1)\n",
    "- Hastie T., Tibshirani R., Friedman J. The Elements of Statistical Learning. Springer: Data Mining, Inference, and Prediction.  — 2nd ed. — Springer-Verlag. 2009. — 746 p.http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf (глава 14)\n",
    "\n",
    "\n",
    "\n",
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2017_fall <номер_группы> <фамилия>``, к примеру -- ``ML2017_fall 496 ivanov``\n",
    "- Выполненное дз сохраните в файл ``<фамилия>_<группа>_task<номер задания>.ipnb``, к примеру -- ``ML2017_496_task1.ipnb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2016_fall Question <Содержание вопроса>``\n",
    "\n",
    "--------\n",
    "- **PS1**: Используются автоматические фильтры, и просто не найдем ваше дз, если вы не аккуратно его подпишите.\n",
    "- **PS2**: Дедлайн жесткий, в том числе помтоу что это ДЗ последнее в курсе. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Контрольные вопросы (0 % - для самоконтроля) </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответе на вопросы своими словами (загугленный материал надо пересказать), ответ обоснуйте (напишите и ОБЪЯСНИТЕ формулки если потребуется), если не выходит, то вернитесь к лекции дополнительным материалам:\n",
    "\n",
    "**Вопрос 1**: В чём заключается проблема мультиколлинеарности?\n",
    "\n",
    "**Ответ**: Явление мультиколинеарности в регрессионной моделе -- это ситуация объекты выборки сосредоточены вблизи линейного подпространства меньшей размерности (это происходит, когда ковариационная матрица имеет близкий к нулю определитель). Из-за этого параметры линейной модели становятся неустойчивыми к малым отклонениям объектов, что влечёт влечёт переобучение и неинтерпретируемость параметров модели.\n",
    "\n",
    "**Вопрос 2**: Какие проблемы при обучении алгоритмов возникают из-за большой размерности пространства признаков?\n",
    "\n",
    "**Ответ**: Проклятие размерности, мультиколлинеарность, и, как следствие, переобучение.\n",
    "\n",
    "**Вопрос 3**: В чем суть проклятия размерности?\n",
    "\n",
    "**Ответ**: Из-за увеличения размерности пространства появляется целая куча проблем: трудоемкость вычислений, необходимость хранения огромного количества данных, увеличение доли шумов, в линейных классификаторах ведёт к проблемам мультиколлинеарности и переобучения. В метрических классификаторах расстояния обычно вычисляются как средний модуль разностей по всем признакам. Согласно Закону Больших Чисел, сумма $n$ слагаемых стремится в некоторому фиксированному пределу при $n\\to\\infty$. Таким образом, расстояния во всех парах объектов стремятся к одному и тому же значению, а значит, становятся неинформативными.\n",
    "\n",
    "** Вопрос 4**: Какая связь между решением задачи PCA и SVD-разложение матрицы регрессии?\n",
    "\n",
    "**Ответ**: Если $m=n$, то PCA даёт представление $F=GU^t$, которое совпадает с сингулярным разложением. Если $m<n$, то представление $F\\approx GU^t$ является приближённым. Сингулярное разложение матрицы $GU^t$ получается из сингулярного разложения матрицы $F$ путём отбрасывания (обнуления) $n − m$ минимальных собственных значений.\n",
    "\n",
    "**Вопрос 6**: На какой идее базируются алгоритмы аггломеративной кластеризации? Напишите формулу Ланса-Вильма\n",
    "\n",
    "**Ответ**: Идея в том, что объекты объединяются друг с другом последовательно, что в итоге приводит к дереву, содержащему всю иерархию исследуемых объектов. Формула Ланса-Вильма (определение расстояния $R(W , S)$\n",
    "между кластерами $W = U \\cup V$ и $S$, зная расстояния $R(U, S)$, $R(V , S)$, $R(U,V )$):\n",
    "$$R(U \\cup V , S) = \\alpha_U R(U, S) + \\alpha_V R(V , S) + \\beta R(U,V ) + \\gamma |R(U, S) − R(V , S)|,$$\n",
    "где $\\alpha_U, \\alpha_V , \\beta, \\gamma$ — числовые параметры.\n",
    "\n",
    "\n",
    "**Вопрос 7**: Какие два шага выделяют в алгоритме кластеризации k-means?\n",
    "\n",
    "**Ответ**: Обновление кластеров по принципу ближайшего соседа и пересчёт центров кластеров (их центра масс). \n",
    "\n",
    "**Вопрос 8**: В чём отличия (основные упрощения) k-means от EM-алгоритма кластеризации?\n",
    "\n",
    "**Ответ**:  Главное отличие в том, что в EM-алгоритме каждый объект $x_i$ распределяется по всем кластерам с некоторыми вероятностями, а в алгоритме k-means каждый объект жёстко приписывается только к одному кластеру. Второе отличие в том, что в k-means форма кластеров не настраивается.\n",
    "\n",
    "** Вопрос 9 **Какой принцип работы графовых алгоритмов кластеризации?\n",
    "\n",
    "**Ответ**: Мы представляем выборку в виде графа. Вершинам графа соответствуют объекты выборки, а рёбрам --\n",
    "попарные расстояния между объектами. Достоинством графовых алгоритмов кластеризации является наглядность, относительная простота реализации, возможность вносить различные усовершенствования, опираясь на простые геометрические соображения.\n",
    "\n",
    "** Вопрос 10 **  В чем некорректность постановки задачи кластеризации?\n",
    "\n",
    "**Ответ**: Решение задачи кластеризации принципиально неоднозначно. Во-первых, не существует однозначно наилучшего критерия качества кластеризации. Известен целый ряд достаточно разумных критериев, а также ряд алгоритмов, не имеющих чётко выраженного критерия, но осуществляющих достаточно разумную кластеризацию «по построению». Все они могут давать разные результаты. Во-вторых, число кластеров, как правило, неизвестно заранее и устанавливается в соответствии с некоторым субъективным критерием. В-третьих, результат кластеризации существенно зависит от метрики, выбор которой, как правило, также субъективен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Вопросы по теории (30%) </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Задача 2 **\n",
    "Даны пять точек на числовой оси $X = (1; 5; 7; 8; 8)$, число кластеров равно 2. Рассчитайте ответ алгоритма  K-means (финальные центры кластеров), если начальные центры кластеров c1 = 1, c2 = 10.\n",
    "\n",
    "** Решение: **\n",
    "Формируем кластеры $(1; 5)$ и $(7; 8; 8)$, относя каждый объект к ближайшему центру. Пересчитываем центры кластеров $c_1=\\frac{1+5}2 = 3$, $c_2=\\frac{7+8+8}3=7\\frac23$. Теперь обновляем кластеры... и видим, что они остаются прежними. Следовательно, алгоритм K-means завершает свою работу. \n",
    "**Ответ:** $c_1=3$, $c_2=7\\frac23$.\n",
    "\n",
    "** Задача 3 **\n",
    "Докажите, что the k-means всегда сходится.\n",
    "\n",
    "** Доказательство: **\n",
    "Докажем, что на каждом шаге алгоритма уменьшается функционал суммарного квадратичного отклонения точек кластеров от центров этих кластеров\n",
    "$$F=\\sum_{y}\\sum_{y_i=y}\\left|f(x_i)-\\mu_y\\right|^2$$\n",
    "Действительно, когда мы обновляем кластеры по принципу ближайшего соседа, то для для каждой точки её отклонение от центра её кластера становится меньше, следовательно и суммы квадратов всех таких отклонений будут меньше. Теперь посмотрим, что происходит с этим функционалов. Для точек $x_i$ каждого кластера $Y_i$ имеем\n",
    "$$\\sum \\left|f(x_i)-\\mu\\right|^2 = \\sum \\left|f(x_i)\\right|^2-2\\left(\\sum f(x_i),\\mu\\right)+|Y_i|\\left|\\mu\\right|^2=\\sum \\left|f(x_i)\\right|^2-\\frac1{|Y_i|}\\left|\\sum f(x_i)\\right| + |Y_i|\\left|\\mu-\\frac{\\sum f(x_i)}{|Y_i|}\\right|^2$$\n",
    "Из этой формулы видно, что минимум суммарного квадратичного отклонения от центра для каждого отдельно взятого кластера достигается при \n",
    "$$\\mu=\\frac{\\sum f(x_i)}{|Y_i|}$$\n",
    "И именно по этой формуле алгоритм K-means обновляет центры кластеров. Что и требовалось доказать.\n",
    "Также понятно, что конечно множество можно лишь конечным числом способов разбить на кластеры; поэтому бесконечно функционал $F$ уменьшать не может и алгоритм когда-нибудь должен будет завершиться!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
